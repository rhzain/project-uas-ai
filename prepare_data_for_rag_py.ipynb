{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\chalkid\\Uni\\Academic\\Sem 4\\Artificial Intelligence\\UAS\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking directory structure for: dataset/SistemOperasi\n",
            "SistemOperasi/\n",
            "  outline_operating_systems.txt\n",
            "  Pertemuan_01_Foundations_Intro/\n",
            "    materi_pertemuan01.txt\n",
            "  Pertemuan_02_Structure_Services_Interaction/\n",
            "    materi_pertemuan02.txt\n",
            "  Pertemuan_03_Process_Management/\n",
            "    materi_pertemuan03.txt\n",
            "  Pertemuan_04_CPU_Scheduling/\n",
            "    materi_pertemuan04.txt\n"
          ]
        }
      ],
      "source": [
        "# Add this diagnostic cell before the main processing\n",
        "def check_directory_structure(base_dir):\n",
        "    \"\"\"Check what files and folders exist in the base directory\"\"\"\n",
        "    print(f\"Checking directory structure for: {base_dir}\")\n",
        "    \n",
        "    if not os.path.exists(base_dir):\n",
        "        print(f\"Base directory '{base_dir}' does not exist!\")\n",
        "        return\n",
        "    \n",
        "    for root, dirs, files in os.walk(base_dir):\n",
        "        level = root.replace(base_dir, '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files:\n",
        "            print(f\"{subindent}{file}\")\n",
        "\n",
        "# Check current structure\n",
        "check_directory_structure(BASE_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-y6n_2Vdd5mN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memulai Prosesor RAG: Persiapan Data...\n",
            "\n",
            "Langkah 1: Mem-parsing outline dari 'dataset/SistemOperasi\\outline_operating_systems.txt'...\n",
            "Berhasil mem-parsing 4 pertemuan dengan file materi dari outline.\n",
            "\n",
            "Langkah 2: Memproses materi per pertemuan...\n",
            "  Memproses: Pertemuan 1 - 'Foundations & Overview of Operating Systems' dari file 'dataset/SistemOperasi\\Pertemuan_01_Foundations_Intro/materi_pertemuan_01.txt'\n",
            "    Dihasilkan 13 chunk untuk pertemuan ini.\n",
            "  Memproses: Pertemuan 2 - 'OS Components, Services, and Structure' dari file 'dataset/SistemOperasi\\Pertemuan_02_Structure_Services_Interaction/materi_pertemuan_02.txt'\n",
            "    Dihasilkan 12 chunk untuk pertemuan ini.\n",
            "  Memproses: Pertemuan 3 - 'Process Management' dari file 'dataset/SistemOperasi\\Pertemuan_03_Process_Management/materi_pertemuan_03.txt'\n",
            "    Dihasilkan 14 chunk untuk pertemuan ini.\n",
            "  Memproses: Pertemuan 4 - 'CPU Scheduling Algorithms' dari file 'dataset/SistemOperasi\\Pertemuan_04_CPU_Scheduling/materi_pertemuan_04.txt'\n",
            "    Dihasilkan 17 chunk untuk pertemuan ini.\n",
            "\n",
            "Total chunk yang diproses dari semua pertemuan: 56\n",
            "\n",
            "Langkah 3: Membuat embeddings untuk semua chunk teks...\n",
            "Memuat model embedding: all-MiniLM-L6-v2...\n",
            "Model embedding berhasil dimuat.\n",
            "Memulai proses embedding untuk 56 chunk teks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 2/2 [00:13<00:00,  6.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Proses embedding selesai. Dihasilkan 56 embeddings dengan dimensi 384.\n",
            "\n",
            "Langkah 4: Membuat dan menyimpan FAISS index...\n",
            "Membuat FAISS index dengan dimensi 384...\n",
            "FAISS index dengan 56 vektor berhasil dibuat dan disimpan ke: dataset/SistemOperasi\\vector_store.index\n",
            "\n",
            "Langkah 5: Menyimpan semua chunk yang diproses (dengan metadata) ke JSON...\n",
            "Semua chunk yang diproses berhasil disimpan ke: dataset/SistemOperasi\\processed_chunks_with_metadata.json\n",
            "\n",
            "--- Prosesor RAG: Persiapan Data Selesai ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Konfigurasi ---\n",
        "BASE_DIR = r\"dataset/SistemOperasi\"  # Direktori utama mata kuliah Anda\n",
        "OUTLINE_FILE = os.path.join(BASE_DIR, \"outline_operating_systems.txt\")\n",
        "OUTPUT_JSON_CHUNKS = os.path.join(BASE_DIR, \"processed_chunks_with_metadata.json\")\n",
        "OUTPUT_FAISS_INDEX = os.path.join(BASE_DIR, \"vector_store.index\")\n",
        "\n",
        "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'  # Menghasilkan vektor 384 dimensi\n",
        "\n",
        "# Parameter untuk chunking\n",
        "MAX_CHUNK_SIZE_CHARS = 1000  # Ukuran maksimal chunk sebelum dipecah lebih lanjut\n",
        "CHUNK_OVERLAP_CHARS = 150   # Jumlah karakter tumpang tindih antar sub-chunk\n",
        "# Pola regex untuk mendeteksi heading (misal: # Judul, ## Sub Judul, dst.)\n",
        "HEADING_SPLIT_PATTERN = r\"(^\\#{1,6}\\s+.*$)\" # Tangkap baris yang dimulai dengan 1-6 '#' diikuti spasi dan teks\n",
        "\n",
        "# --- Fungsi Helper ---\n",
        "\n",
        "def parse_outline(outline_filepath):\n",
        "    \"\"\"\n",
        "    Mem-parsing file outline untuk mendapatkan informasi setiap pertemuan.\n",
        "    Mengasumsikan format KEY: VALUE dan pemisah antar pertemuan adalah baris kosong.\n",
        "    \"\"\"\n",
        "    pertemuan_list = []\n",
        "    try:\n",
        "        with open(outline_filepath, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # Pisahkan berdasarkan blok pertemuan (diasumsikan dipisah oleh 'PERTEMUAN:')\n",
        "        # dan pastikan ada MATAKULIAH di awal\n",
        "        if not content.strip().startswith(\"MATAKULIAH:\"):\n",
        "            print(f\"Peringatan: Format file outline '{outline_filepath}' mungkin tidak sesuai (tidak ada 'MATAKULIAH:').\")\n",
        "            # return [] # Bisa dihentikan jika format ketat\n",
        "\n",
        "        # Menggunakan regex untuk menangkap blok pertemuan dengan lebih fleksibel\n",
        "        # Pola ini mencari \"PERTEMUAN:\" dan mengambil semua baris hingga \"PERTEMUAN:\" berikutnya atau akhir file\n",
        "        pertemuan_blocks = re.split(r'\\nPERTEMUAN:', '\\n' + content.split('PERTEMUAN:', 1)[-1] if 'PERTEMUAN:' in content else '')\n",
        "\n",
        "        for block in pertemuan_blocks:\n",
        "            if not block.strip():\n",
        "                continue\n",
        "\n",
        "            current_pertemuan = {}\n",
        "            lines = block.strip().splitlines()\n",
        "\n",
        "            # Ambil ID Pertemuan dari baris pertama blok (setelah \"PERTEMUAN:\")\n",
        "            if lines:\n",
        "                pertemuan_id_match = re.match(r'^\\s*(\\d+)', lines[0])\n",
        "                if pertemuan_id_match:\n",
        "                    current_pertemuan['id'] = int(pertemuan_id_match.group(1))\n",
        "                else:\n",
        "                    print(f\"Peringatan: Tidak bisa parse ID Pertemuan dari blok: {lines[0]}\")\n",
        "                    continue # Lewati blok ini jika ID tidak bisa diparse\n",
        "\n",
        "                # Proses sisa baris untuk KEY: VALUE\n",
        "                for line in lines: # Mulai dari baris pertama lagi untuk key lain juga\n",
        "                    if \":\" in line:\n",
        "                        key, value = line.split(\":\", 1)\n",
        "                        key = key.strip().lower().replace(\" \", \"_\")\n",
        "                        value = value.strip()\n",
        "                        if key == \"file_materi\" and not value: # Jika FILE_MATERI kosong\n",
        "                            current_pertemuan[key] = None\n",
        "                        else:\n",
        "                            current_pertemuan[key] = value\n",
        "\n",
        "            if 'id' in current_pertemuan and 'judul' in current_pertemuan and 'file_materi' in current_pertemuan :\n",
        "                 # Pastikan file materi tidak None atau string kosong untuk dimasukkan\n",
        "                if current_pertemuan.get('file_materi'):\n",
        "                    pertemuan_list.append(current_pertemuan)\n",
        "                elif current_pertemuan.get('file_materi') is None or not current_pertemuan.get('file_materi','').strip() :\n",
        "                    print(f\"Info: Pertemuan {current_pertemuan.get('id','N/A')} ({current_pertemuan.get('judul','Tanpa Judul')}) tidak memiliki file materi, akan dilewati untuk RAG.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File outline '{outline_filepath}' tidak ditemukan.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saat mem-parsing file outline: {e}\")\n",
        "\n",
        "    print(f\"Berhasil mem-parsing {len(pertemuan_list)} pertemuan dengan file materi dari outline.\")\n",
        "    return pertemuan_list\n",
        "\n",
        "\n",
        "def read_material_text(material_filepath):\n",
        "    \"\"\"Membaca konten teks dari file.\"\"\"\n",
        "    try:\n",
        "        with open(material_filepath, 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File materi '{material_filepath}' tidak ditemukan.\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error saat membaca file materi '{material_filepath}': {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def _split_text_block_sliding_window(text_block, pertemuan_id, pertemuan_judul, heading, max_size, overlap):\n",
        "    \"\"\"\n",
        "    Helper untuk memecah blok teks yang panjang menggunakan sliding window karakter.\n",
        "    \"\"\"\n",
        "    final_chunks = []\n",
        "    text_block_stripped = text_block.strip()\n",
        "    if not text_block_stripped:\n",
        "        return []\n",
        "\n",
        "    if len(text_block_stripped) <= max_size:\n",
        "        final_chunks.append({\n",
        "            \"pertemuan_id\": pertemuan_id,\n",
        "            \"pertemuan_judul\": pertemuan_judul,\n",
        "            \"original_heading\": heading,\n",
        "            \"chunk_text\": text_block_stripped\n",
        "        })\n",
        "    else:\n",
        "        start_index = 0\n",
        "        doc_len = len(text_block_stripped)\n",
        "        while start_index < doc_len:\n",
        "            end_index = start_index + max_size\n",
        "            current_slice = text_block_stripped[start_index:min(end_index, doc_len)]\n",
        "\n",
        "            chunk_text_final = current_slice.strip()\n",
        "\n",
        "            if chunk_text_final:\n",
        "                final_chunks.append({\n",
        "                    \"pertemuan_id\": pertemuan_id,\n",
        "                    \"pertemuan_judul\": pertemuan_judul,\n",
        "                    \"original_heading\": heading,\n",
        "                    \"chunk_text\": chunk_text_final\n",
        "                })\n",
        "\n",
        "            if min(end_index, doc_len) >= doc_len:\n",
        "                break\n",
        "\n",
        "            start_index += (max_size - overlap)\n",
        "            start_index = max(0, start_index)\n",
        "            if start_index >= doc_len:\n",
        "                break\n",
        "\n",
        "    return final_chunks\n",
        "\n",
        "def chunk_material_heading_aware(text_content, pertemuan_id, pertemuan_judul):\n",
        "    \"\"\"\n",
        "    Memecah konten materi menjadi chunks, mempertimbangkan heading sebagai pemisah alami.\n",
        "    Jika teks di bawah satu heading terlalu panjang, akan dipecah lebih lanjut.\n",
        "    \"\"\"\n",
        "    processed_chunks = []\n",
        "    if not text_content or not text_content.strip():\n",
        "        return []\n",
        "\n",
        "    # Pisahkan teks berdasarkan heading, sambil mempertahankan headingnya.\n",
        "    # re.split dengan capturing group (...) akan mempertahankan delimiter.\n",
        "    parts = re.split(HEADING_SPLIT_PATTERN, text_content, flags=re.MULTILINE)\n",
        "\n",
        "    current_heading = \"Umum\" # Default untuk konten sebelum heading pertama\n",
        "    accumulated_text_for_section = \"\"\n",
        "\n",
        "    for i, part in enumerate(parts):\n",
        "        part_stripped = part.strip()\n",
        "        if not part_stripped:\n",
        "            continue\n",
        "\n",
        "        # Cek apakah part ini adalah heading (berdasarkan pola regex)\n",
        "        is_current_part_a_heading = re.match(HEADING_SPLIT_PATTERN, part_stripped, flags=re.MULTILINE)\n",
        "\n",
        "        if is_current_part_a_heading:\n",
        "            # Jika ada teks yang sudah terakumulasi untuk section SEBELUMNYA, proses dulu\n",
        "            if accumulated_text_for_section.strip():\n",
        "                sub_chunks = _split_text_block_sliding_window(accumulated_text_for_section,\n",
        "                                                              pertemuan_id, pertemuan_judul, current_heading,\n",
        "                                                              MAX_CHUNK_SIZE_CHARS, CHUNK_OVERLAP_CHARS)\n",
        "                processed_chunks.extend(sub_chunks)\n",
        "\n",
        "            current_heading = part_stripped  # Update heading saat ini\n",
        "            accumulated_text_for_section = \"\" # Reset akumulator teks\n",
        "        else:\n",
        "            # Ini adalah konten di bawah heading saat ini\n",
        "            accumulated_text_for_section += part_stripped + \"\\n\" # Tambahkan newline agar antar paragraf tidak menyatu\n",
        "\n",
        "    # Proses sisa teks yang terakumulasi untuk section terakhir\n",
        "    if accumulated_text_for_section.strip():\n",
        "        sub_chunks = _split_text_block_sliding_window(accumulated_text_for_section,\n",
        "                                                      pertemuan_id, pertemuan_judul, current_heading,\n",
        "                                                      MAX_CHUNK_SIZE_CHARS, CHUNK_OVERLAP_CHARS)\n",
        "        processed_chunks.extend(sub_chunks)\n",
        "\n",
        "    return processed_chunks\n",
        "\n",
        "def get_text_embeddings(list_of_chunk_texts, model_name=EMBEDDING_MODEL_NAME):\n",
        "    \"\"\"Mengubah daftar teks chunk menjadi vektor embeddings.\"\"\"\n",
        "    if not list_of_chunk_texts:\n",
        "        print(\"Tidak ada teks untuk di-embed.\")\n",
        "        return np.array([])\n",
        "    try:\n",
        "        print(f\"Memuat model embedding: {model_name}...\")\n",
        "        embedding_model = SentenceTransformer(model_name)\n",
        "        print(\"Model embedding berhasil dimuat.\")\n",
        "        print(f\"Memulai proses embedding untuk {len(list_of_chunk_texts)} chunk teks...\")\n",
        "        embeddings = embedding_model.encode(list_of_chunk_texts, show_progress_bar=True)\n",
        "        print(f\"Proses embedding selesai. Dihasilkan {embeddings.shape[0]} embeddings dengan dimensi {embeddings.shape[1]}.\")\n",
        "        return embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"Error saat membuat embeddings: {e}\")\n",
        "        return np.array([])\n",
        "\n",
        "def create_and_save_faiss_index(embeddings_np_array, index_output_path=OUTPUT_FAISS_INDEX):\n",
        "    \"\"\"Membuat FAISS index dan menyimpannya.\"\"\"\n",
        "    if embeddings_np_array.size == 0 or embeddings_np_array.ndim != 2:\n",
        "        print(\"Array embedding kosong atau formatnya salah. FAISS index tidak dibuat.\")\n",
        "        return\n",
        "    dimension = embeddings_np_array.shape[1]\n",
        "    try:\n",
        "        print(f\"Membuat FAISS index dengan dimensi {dimension}...\")\n",
        "        index = faiss.IndexFlatL2(dimension)\n",
        "        index.add(embeddings_np_array.astype('float32'))\n",
        "        faiss.write_index(index, index_output_path)\n",
        "        print(f\"FAISS index dengan {index.ntotal} vektor berhasil dibuat dan disimpan ke: {index_output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saat membuat atau menyimpan FAISS index: {e}\")\n",
        "\n",
        "# --- Proses Utama Skrip ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Memulai Prosesor RAG: Persiapan Data...\")\n",
        "\n",
        "    # 1. Parse file outline mata kuliah\n",
        "    print(f\"\\nLangkah 1: Mem-parsing outline dari '{OUTLINE_FILE}'...\")\n",
        "    daftar_pertemuan = parse_outline(OUTLINE_FILE)\n",
        "\n",
        "    if not daftar_pertemuan:\n",
        "        print(\"Tidak ada informasi pertemuan yang valid dari outline. Proses dihentikan.\")\n",
        "    else:\n",
        "        all_processed_chunks_with_metadata = []\n",
        "\n",
        "        # 2. Loop setiap pertemuan untuk membaca materi dan melakukan chunking\n",
        "        print(f\"\\nLangkah 2: Memproses materi per pertemuan...\")\n",
        "        for pertemuan_info in daftar_pertemuan:\n",
        "            pertemuan_id = pertemuan_info.get('id')\n",
        "            judul_pertemuan = pertemuan_info.get('judul', f\"Pertemuan {pertemuan_id}\")\n",
        "            file_materi_rel_path = pertemuan_info.get('file_materi')\n",
        "\n",
        "            if not file_materi_rel_path:\n",
        "                print(f\"Info: Pertemuan ID {pertemuan_id} ({judul_pertemuan}) tidak memiliki path file materi. Dilewati.\")\n",
        "                continue\n",
        "\n",
        "            file_materi_abs_path = os.path.join(BASE_DIR, file_materi_rel_path)\n",
        "            print(f\"  Memproses: Pertemuan {pertemuan_id} - '{judul_pertemuan}' dari file '{file_materi_abs_path}'\")\n",
        "\n",
        "            materi_text = read_material_text(file_materi_abs_path)\n",
        "            if materi_text:\n",
        "                chunks_for_this_pertemuan = chunk_material_heading_aware(materi_text, pertemuan_id, judul_pertemuan)\n",
        "                all_processed_chunks_with_metadata.extend(chunks_for_this_pertemuan)\n",
        "                print(f\"    Dihasilkan {len(chunks_for_this_pertemuan)} chunk untuk pertemuan ini.\")\n",
        "            else:\n",
        "                print(f\"    Tidak ada konten teks yang dibaca dari '{file_materi_abs_path}'.\")\n",
        "\n",
        "        print(f\"\\nTotal chunk yang diproses dari semua pertemuan: {len(all_processed_chunks_with_metadata)}\")\n",
        "\n",
        "        if all_processed_chunks_with_metadata:\n",
        "            # Ekstrak hanya teks chunk untuk proses embedding\n",
        "            list_of_chunk_texts_for_embedding = [chunk['chunk_text'] for chunk in all_processed_chunks_with_metadata]\n",
        "\n",
        "            # 3. Buat Embeddings\n",
        "            print(f\"\\nLangkah 3: Membuat embeddings untuk semua chunk teks...\")\n",
        "            document_embeddings = get_text_embeddings(list_of_chunk_texts_for_embedding)\n",
        "\n",
        "            if document_embeddings.size > 0:\n",
        "                # 4. Buat dan Simpan FAISS Index\n",
        "                print(f\"\\nLangkah 4: Membuat dan menyimpan FAISS index...\")\n",
        "                create_and_save_faiss_index(document_embeddings, OUTPUT_FAISS_INDEX)\n",
        "\n",
        "                # 5. Simpan semua chunks beserta metadatanya ke file JSON\n",
        "                print(f\"\\nLangkah 5: Menyimpan semua chunk yang diproses (dengan metadata) ke JSON...\")\n",
        "                try:\n",
        "                    with open(OUTPUT_JSON_CHUNKS, \"w\", encoding=\"utf-8\") as f:\n",
        "                        json.dump(all_processed_chunks_with_metadata, f, ensure_ascii=False, indent=2)\n",
        "                    print(f\"Semua chunk yang diproses berhasil disimpan ke: {OUTPUT_JSON_CHUNKS}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error saat menyimpan chunks ke JSON: {e}\")\n",
        "            else:\n",
        "                print(\"Pembuatan FAISS index dan penyimpanan JSON chunks dibatalkan karena tidak ada embeddings yang valid.\")\n",
        "        else:\n",
        "            print(\"Tidak ada chunk yang diproses sama sekali. Pastikan file materi ada dan berisi teks.\")\n",
        "\n",
        "    print(\"\\n--- Prosesor RAG: Persiapan Data Selesai ---\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
